[TOC]

# Deep Learning

**梯度消失**：主要是指激活函数的导数小于1，例如sigmoid函数，tanh

**梯度爆炸**:相反是指激活函数的梯度大于1

如果激活函数是**sigmoid**就会产生梯度消失，**tanh**也是一样，他们两个的梯度都小于1，而**relu**函数梯度等于1解决了梯度消失问题

elu、Gelu

Instance Segmentation

backbone

head Architecture

支持向量机SVM

损失函数针对单个样本

代价函数针对整个训练集

## RNN

![v2-f716c816d46792b867a6815c278f11cb_r](深度学习.assets/v2-f716c816d46792b867a6815c278f11cb_r.jpg)![v2-71652d6a1eee9def631c18ea5e3c7605_720w](深度学习.assets/v2-71652d6a1eee9def631c18ea5e3c7605_720w.jpg)![v2-556c74f0e025a47fea05dc0f76ea775d_r](深度学习.assets/v2-556c74f0e025a47fea05dc0f76ea775d_r.jpg)



## 反卷积

## 上采样和反卷积

## SE block

## t-SNE

## notes

高维空间数据都是稀疏的

![image-20210602165318880](深度学习.assets/image-20210602165318880.png)

![image-20210602165936214](深度学习.assets/image-20210602165936214.png)

## meta learning

![image-20210603202823342](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210603202823342.png)

![image-20210603202749476](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210603202749476.png)

![image-20210603202846918](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210603202846918.png)

拉普拉斯先验

高斯先验对应L2正则项

![image-20210607152430740](深度学习.assets/image-20210607152430740.png)

## Welfold Algorithm

![image-20210704125814840](深度学习.assets/image-20210704125814840-16410044721841.png)

## GMM

## 稀疏自编码器

![image-20210707195221119](深度学习.assets/image-20210707195221119.png)

## GAN

![image-20210710151236984](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210710151236984.png)

![image-20210710151249945](深度学习.assets/image-20210710151249945.png)

https://jonathan-hui.medium.com/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b

## DCGAN

## WGAN-GP

https://zhuanlan.zhihu.com/p/58260684

![img](深度学习.assets/v2-234febde646b07c96c5a02a4dc43947f_1440w.jpg)

注意，用于计算GP的样本是生成样本和真实样本的线性插值。

## Conditional GAN

https://zhuanlan.zhihu.com/p/86451268

## JS散度

## Hungarian Algorithm

匈牙利算法

https://blog.csdn.net/u011837761/article/details/52058703

## Bilinear CNN

## InfoGAN

## StyleGAN

## VAEGAN

## transformer参数量计算

https://www.jianshu.com/p/677fc8be5e14

![image-20211021140950132](深度学习.assets/image-20211021140950132.png)

## 为什么self attention要scale

https://blog.csdn.net/weixin_37947156/article/details/100082543

## MLP、CNN、RNN、Transformer处理序列任务

![image-20211022135729105](深度学习.assets/image-20211022135729105.png)

## 自回归和自编码模型

